#!/usr/bin/env python3
"""
PCEæœ€ä¼˜é˜¶æ•°é€‰æ‹©å·¥å…·
é€šè¿‡åˆ†æå‡½æ•°éçº¿æ€§å¼ºåº¦æ¥å†³å®šPCEçš„æœ€ä¼˜å¤šé¡¹å¼é˜¶æ•°

ç†è®ºåŸºç¡€ï¼š
1. éçº¿æ€§å¼ºåº¦æŒ‡æ ‡
2. ä¿¡æ¯è®ºå‡†åˆ™ (AIC, BIC)
3. äº¤å‰éªŒè¯
4. æ ·æœ¬å¤æ‚åº¦åˆ†æ
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
import pandas as pd
import seaborn as sns
from scipy import stats
from scipy.fft import fft, fftfreq
import warnings
warnings.filterwarnings('ignore')

class NonlinearityAnalyzer:
    """éçº¿æ€§å¼ºåº¦åˆ†æå™¨"""
    
    def __init__(self, X, Y):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            X: è¾“å…¥æ•°æ® (n_samples, n_features)
            Y: è¾“å‡ºæ•°æ® (n_samples, n_outputs)
        """
        self.X = X
        self.Y = Y
        self.n_samples, self.n_features = X.shape
        self.n_outputs = Y.shape[1] if Y.ndim > 1 else 1
        
        # æ ‡å‡†åŒ–æ•°æ®
        self.scaler_X = StandardScaler()
        self.scaler_Y = StandardScaler()
        self.X_scaled = self.scaler_X.fit_transform(X)
        self.Y_scaled = self.scaler_Y.fit_transform(Y.reshape(-1, 1) if Y.ndim == 1 else Y)
        
    def compute_nonlinearity_metrics(self):
        """è®¡ç®—å¤šç§éçº¿æ€§å¼ºåº¦æŒ‡æ ‡"""
        metrics = {}
        
        # 1. çº¿æ€§ç›¸å…³æ€§åˆ†æ
        metrics['linear_correlation'] = self._linear_correlation_analysis()
        
        # 2. é«˜é˜¶çŸ©åˆ†æ
        metrics['higher_moments'] = self._higher_moments_analysis()
        
        # 3. é¢‘åŸŸåˆ†æ
        metrics['frequency_analysis'] = self._frequency_domain_analysis()
        
        # 4. å±€éƒ¨çº¿æ€§åº¦åˆ†æ
        metrics['local_linearity'] = self._local_linearity_analysis()
        
        # 5. æ¢¯åº¦å˜åŒ–åˆ†æ
        metrics['gradient_variation'] = self._gradient_variation_analysis()
        
        return metrics
    
    def _linear_correlation_analysis(self):
        """çº¿æ€§ç›¸å…³æ€§åˆ†æ"""
        correlations = []
        
        for i in range(self.n_outputs):
            y = self.Y_scaled[:, i] if self.Y_scaled.ndim > 1 else self.Y_scaled
            
            # è®¡ç®—ä¸æ¯ä¸ªè¾“å…¥çš„çº¿æ€§ç›¸å…³æ€§
            corr_with_inputs = []
            for j in range(self.n_features):
                corr = np.corrcoef(self.X_scaled[:, j], y)[0, 1]
                corr_with_inputs.append(abs(corr))
            
            # è®¡ç®—ä¸è¾“å…¥ä¹˜ç§¯é¡¹çš„ç›¸å…³æ€§
            interaction_corrs = []
            for j in range(self.n_features):
                for k in range(j+1, self.n_features):
                    interaction = self.X_scaled[:, j] * self.X_scaled[:, k]
                    corr = np.corrcoef(interaction, y)[0, 1]
                    interaction_corrs.append(abs(corr))
            
            # è®¡ç®—ä¸å¹³æ–¹é¡¹çš„ç›¸å…³æ€§
            quadratic_corrs = []
            for j in range(self.n_features):
                quadratic = self.X_scaled[:, j] ** 2
                corr = np.corrcoef(quadratic, y)[0, 1]
                quadratic_corrs.append(abs(corr))
            
            correlations.append({
                'linear': np.mean(corr_with_inputs),
                'interaction': np.mean(interaction_corrs) if interaction_corrs else 0,
                'quadratic': np.mean(quadratic_corrs)
            })
        
        return {
            'avg_linear_corr': np.mean([c['linear'] for c in correlations]),
            'avg_interaction_corr': np.mean([c['interaction'] for c in correlations]),
            'avg_quadratic_corr': np.mean([c['quadratic'] for c in correlations]),
            'nonlinearity_ratio': np.mean([c['quadratic'] + c['interaction'] for c in correlations]) / 
                                 (np.mean([c['linear'] for c in correlations]) + 1e-8)
        }
    
    def _higher_moments_analysis(self):
        """é«˜é˜¶çŸ©åˆ†æ"""
        moments = {}
        
        for i in range(self.n_outputs):
            y = self.Y_scaled[:, i] if self.Y_scaled.ndim > 1 else self.Y_scaled
            
            # è®¡ç®—ååº¦å’Œå³°åº¦
            skewness = stats.skew(y)
            kurtosis = stats.kurtosis(y)
            
            moments[f'output_{i}'] = {
                'skewness': abs(skewness),
                'kurtosis': abs(kurtosis)
            }
        
        avg_skewness = np.mean([m['skewness'] for m in moments.values()])
        avg_kurtosis = np.mean([m['kurtosis'] for m in moments.values()])
        
        return {
            'avg_skewness': avg_skewness,
            'avg_kurtosis': avg_kurtosis,
            'moment_complexity': avg_skewness + avg_kurtosis
        }
    
    def _frequency_domain_analysis(self):
        """é¢‘åŸŸåˆ†æ - æ£€æµ‹é«˜é¢‘æˆåˆ†"""
        high_freq_ratios = []
        
        for i in range(self.n_outputs):
            y = self.Y_scaled[:, i] if self.Y_scaled.ndim > 1 else self.Y_scaled
            
            # FFTåˆ†æ
            fft_y = fft(y)
            freqs = fftfreq(len(y))
            
            # è®¡ç®—é«˜é¢‘æˆåˆ†æ¯”ä¾‹
            power_spectrum = np.abs(fft_y) ** 2
            total_power = np.sum(power_spectrum)
            
            # å®šä¹‰é«˜é¢‘ä¸ºé¢‘ç‡ > 0.1 (å½’ä¸€åŒ–é¢‘ç‡)
            high_freq_mask = np.abs(freqs) > 0.1
            high_freq_power = np.sum(power_spectrum[high_freq_mask])
            
            high_freq_ratio = high_freq_power / (total_power + 1e-8)
            high_freq_ratios.append(high_freq_ratio)
        
        return {
            'avg_high_freq_ratio': np.mean(high_freq_ratios),
            'max_high_freq_ratio': np.max(high_freq_ratios)
        }
    
    def _local_linearity_analysis(self):
        """å±€éƒ¨çº¿æ€§åº¦åˆ†æ"""
        # ä½¿ç”¨kè¿‘é‚»åˆ†æå±€éƒ¨çº¿æ€§åº¦
        from sklearn.neighbors import NearestNeighbors
        
        k = min(10, self.n_samples // 4)  # é€‰æ‹©åˆé€‚çš„kå€¼
        nbrs = NearestNeighbors(n_neighbors=k).fit(self.X_scaled)
        
        local_linearities = []
        
        for i in range(min(100, self.n_samples)):  # é‡‡æ ·åˆ†æ
            # æ‰¾åˆ°kä¸ªæœ€è¿‘é‚»
            distances, indices = nbrs.kneighbors([self.X_scaled[i]])
            
            # åœ¨å±€éƒ¨åŒºåŸŸæ‹Ÿåˆçº¿æ€§æ¨¡å‹
            local_X = self.X_scaled[indices[0]]
            local_Y = self.Y_scaled[indices[0]] if self.Y_scaled.ndim > 1 else self.Y_scaled[indices[0]]
            
            # è®¡ç®—å±€éƒ¨çº¿æ€§æ‹Ÿåˆçš„RÂ²
            if local_Y.ndim == 1:
                local_Y = local_Y.reshape(-1, 1)
            
            local_r2_scores = []
            for j in range(local_Y.shape[1]):
                try:
                    from sklearn.linear_model import LinearRegression
                    lr = LinearRegression()
                    lr.fit(local_X, local_Y[:, j])
                    local_pred = lr.predict(local_X)
                    local_r2 = r2_score(local_Y[:, j], local_pred)
                    local_r2_scores.append(max(0, local_r2))  # é¿å…è´Ÿå€¼
                except:
                    local_r2_scores.append(0)
            
            local_linearities.append(np.mean(local_r2_scores))
        
        return {
            'avg_local_linearity': np.mean(local_linearities),
            'min_local_linearity': np.min(local_linearities)
        }
    
    def _gradient_variation_analysis(self):
        """æ¢¯åº¦å˜åŒ–åˆ†æ"""
        # è®¡ç®—æ•°å€¼æ¢¯åº¦çš„å˜åŒ–
        gradients = []
        
        for i in range(self.n_outputs):
            y = self.Y_scaled[:, i] if self.Y_scaled.ndim > 1 else self.Y_scaled
            
            # è®¡ç®—æ¯ä¸ªç»´åº¦çš„æ¢¯åº¦
            grad_variations = []
            for j in range(self.n_features):
                # æ’åºä»¥è®¡ç®—æ¢¯åº¦
                sorted_indices = np.argsort(self.X_scaled[:, j])
                sorted_x = self.X_scaled[sorted_indices, j]
                sorted_y = y[sorted_indices]
                
                # è®¡ç®—æ•°å€¼æ¢¯åº¦
                grad = np.gradient(sorted_y, sorted_x)
                
                # è®¡ç®—æ¢¯åº¦çš„å˜åŒ–ç¨‹åº¦
                grad_variation = np.std(grad) / (np.mean(np.abs(grad)) + 1e-8)
                grad_variations.append(grad_variation)
            
            gradients.append(np.mean(grad_variations))
        
        return {
            'avg_gradient_variation': np.mean(gradients),
            'max_gradient_variation': np.max(gradients)
        }

class PCEOrderSelector:
    """PCEé˜¶æ•°é€‰æ‹©å™¨"""
    
    def __init__(self, X, Y, max_order=5):
        """
        åˆå§‹åŒ–é€‰æ‹©å™¨
        
        Args:
            X: è¾“å…¥æ•°æ®
            Y: è¾“å‡ºæ•°æ®
            max_order: æœ€å¤§æµ‹è¯•é˜¶æ•°
        """
        self.X = X
        self.Y = Y
        self.max_order = max_order
        self.analyzer = NonlinearityAnalyzer(X, Y)
        
    def select_optimal_order(self):
        """é€‰æ‹©æœ€ä¼˜é˜¶æ•°"""
        print("ğŸ” åˆ†æå‡½æ•°éçº¿æ€§å¼ºåº¦...")
        
        # 1. éçº¿æ€§å¼ºåº¦åˆ†æ
        nonlinearity_metrics = self.analyzer.compute_nonlinearity_metrics()
        
        # 2. åŸºäºéçº¿æ€§å¼ºåº¦çš„åˆæ­¥å»ºè®®
        theory_based_order = self._theory_based_order_selection(nonlinearity_metrics)
        
        # 3. äº¤å‰éªŒè¯é€‰æ‹©
        cv_results = self._cross_validation_order_selection()
        
        # 4. ä¿¡æ¯å‡†åˆ™é€‰æ‹©
        ic_results = self._information_criterion_selection()
        
        # 5. ç»¼åˆå†³ç­–
        optimal_order = self._make_final_decision(theory_based_order, cv_results, ic_results)
        
        return {
            'optimal_order': optimal_order,
            'nonlinearity_metrics': nonlinearity_metrics,
            'theory_based_order': theory_based_order,
            'cv_results': cv_results,
            'ic_results': ic_results
        }
    
    def _theory_based_order_selection(self, metrics):
        """åŸºäºç†è®ºçš„é˜¶æ•°é€‰æ‹©"""
        print("ğŸ“Š åŸºäºéçº¿æ€§å¼ºåº¦ç†è®ºåˆ†æ...")
        
        # è®¡ç®—ç»¼åˆéçº¿æ€§å¼ºåº¦åˆ†æ•°
        linear_corr = metrics['linear_correlation']['avg_linear_corr']
        nonlin_ratio = metrics['linear_correlation']['nonlinearity_ratio']
        moment_complexity = metrics['higher_moments']['moment_complexity']
        high_freq_ratio = metrics['frequency_analysis']['avg_high_freq_ratio']
        local_linearity = metrics['local_linearity']['avg_local_linearity']
        grad_variation = metrics['gradient_variation']['avg_gradient_variation']
        
        # å½’ä¸€åŒ–æŒ‡æ ‡
        nonlinearity_score = (
            (1 - linear_corr) * 0.2 +           # çº¿æ€§ç›¸å…³æ€§è¶Šä½ï¼Œéçº¿æ€§è¶Šå¼º
            min(nonlin_ratio, 2.0) * 0.2 +      # éçº¿æ€§æ¯”ä¾‹
            min(moment_complexity, 5.0) * 0.15 + # é«˜é˜¶çŸ©å¤æ‚åº¦
            min(high_freq_ratio, 1.0) * 0.2 +   # é«˜é¢‘æˆåˆ†
            (1 - local_linearity) * 0.15 +      # å±€éƒ¨éçº¿æ€§
            min(grad_variation, 3.0) * 0.1      # æ¢¯åº¦å˜åŒ–
        )
        
        print(f"   éçº¿æ€§å¼ºåº¦åˆ†æ•°: {nonlinearity_score:.3f}")
        
        # åŸºäºåˆ†æ•°å»ºè®®é˜¶æ•°
        if nonlinearity_score < 0.3:
            suggested_order = 1
            reason = "å‡½æ•°æ¥è¿‘çº¿æ€§"
        elif nonlinearity_score < 0.6:
            suggested_order = 2
            reason = "ä¸­ç­‰éçº¿æ€§å¼ºåº¦"
        elif nonlinearity_score < 1.0:
            suggested_order = 3
            reason = "è¾ƒå¼ºéçº¿æ€§"
        elif nonlinearity_score < 1.5:
            suggested_order = 4
            reason = "å¼ºéçº¿æ€§"
        else:
            suggested_order = 5
            reason = "æå¼ºéçº¿æ€§"
        
        print(f"   ç†è®ºå»ºè®®é˜¶æ•°: {suggested_order} ({reason})")
        
        return {
            'suggested_order': suggested_order,
            'nonlinearity_score': nonlinearity_score,
            'reason': reason,
            'detailed_scores': {
                'linear_correlation': linear_corr,
                'nonlinearity_ratio': nonlin_ratio,
                'moment_complexity': moment_complexity,
                'high_frequency_ratio': high_freq_ratio,
                'local_linearity': local_linearity,
                'gradient_variation': grad_variation
            }
        }
    
    def _cross_validation_order_selection(self):
        """äº¤å‰éªŒè¯é€‰æ‹©é˜¶æ•°"""
        print("ğŸ”„ äº¤å‰éªŒè¯æµ‹è¯•ä¸åŒé˜¶æ•°...")
        
        orders = range(1, min(self.max_order + 1, 6))  # é™åˆ¶æœ€å¤§é˜¶æ•°
        cv_scores = {}
        
        kf = KFold(n_splits=5, shuffle=True, random_state=42)
        
        for order in orders:
            scores = []
            
            for train_idx, val_idx in kf.split(self.X):
                X_train, X_val = self.X[train_idx], self.X[val_idx]
                Y_train, Y_val = self.Y[train_idx], self.Y[val_idx]
                
                try:
                    # åˆ›å»ºç®€åŒ–çš„PCEæ¨¡å‹
                    pce_score = self._fit_evaluate_pce(X_train, Y_train, X_val, Y_val, order)
                    scores.append(pce_score)
                except Exception as e:
                    print(f"   è­¦å‘Š: é˜¶æ•°{order}æµ‹è¯•å¤±è´¥: {e}")
                    scores.append(0)
            
            cv_scores[order] = {
                'mean_score': np.mean(scores),
                'std_score': np.std(scores),
                'scores': scores
            }
            
            print(f"   é˜¶æ•°{order}: RÂ² = {np.mean(scores):.3f} Â± {np.std(scores):.3f}")
        
        # é€‰æ‹©æœ€ä½³é˜¶æ•°
        best_order = max(cv_scores.keys(), key=lambda k: cv_scores[k]['mean_score'])
        
        return {
            'best_order': best_order,
            'cv_scores': cv_scores
        }
    
    def _fit_evaluate_pce(self, X_train, Y_train, X_val, Y_val, order):
        """æ‹Ÿåˆå’Œè¯„ä¼°PCEæ¨¡å‹"""
        from sklearn.preprocessing import PolynomialFeatures
        from sklearn.linear_model import Ridge
        
        # åˆ›å»ºå¤šé¡¹å¼ç‰¹å¾
        poly = PolynomialFeatures(degree=order, include_bias=True)
        X_train_poly = poly.fit_transform(X_train)
        X_val_poly = poly.transform(X_val)
        
        # è®­ç»ƒæ¨¡å‹
        model = Ridge(alpha=1e-6)
        
        if Y_train.ndim == 1:
            model.fit(X_train_poly, Y_train)
            Y_pred = model.predict(X_val_poly)
            return r2_score(Y_val, Y_pred)
        else:
            scores = []
            for i in range(Y_train.shape[1]):
                model.fit(X_train_poly, Y_train[:, i])
                Y_pred = model.predict(X_val_poly)
                score = r2_score(Y_val[:, i], Y_pred)
                scores.append(max(0, score))  # é¿å…è´Ÿåˆ†æ•°
            return np.mean(scores)
    
    def _information_criterion_selection(self):
        """ä¿¡æ¯å‡†åˆ™é€‰æ‹©"""
        print("ğŸ“ˆ ä¿¡æ¯å‡†åˆ™åˆ†æ...")
        
        orders = range(1, min(self.max_order + 1, 6))
        ic_results = {}
        
        for order in orders:
            try:
                aic, bic = self._compute_information_criteria(order)
                ic_results[order] = {'AIC': aic, 'BIC': bic}
                print(f"   é˜¶æ•°{order}: AIC = {aic:.2f}, BIC = {bic:.2f}")
            except Exception as e:
                print(f"   è­¦å‘Š: é˜¶æ•°{order}ä¿¡æ¯å‡†åˆ™è®¡ç®—å¤±è´¥: {e}")
                ic_results[order] = {'AIC': float('inf'), 'BIC': float('inf')}
        
        # é€‰æ‹©AICå’ŒBICæœ€å°çš„é˜¶æ•°
        best_aic_order = min(ic_results.keys(), key=lambda k: ic_results[k]['AIC'])
        best_bic_order = min(ic_results.keys(), key=lambda k: ic_results[k]['BIC'])
        
        return {
            'best_aic_order': best_aic_order,
            'best_bic_order': best_bic_order,
            'ic_results': ic_results
        }
    
    def _compute_information_criteria(self, order):
        """è®¡ç®—AICå’ŒBIC"""
        from sklearn.preprocessing import PolynomialFeatures
        from sklearn.linear_model import LinearRegression
        
        # åˆ›å»ºå¤šé¡¹å¼ç‰¹å¾
        poly = PolynomialFeatures(degree=order, include_bias=True)
        X_poly = poly.fit_transform(self.X)
        
        n_samples, n_features = X_poly.shape
        
        # æ‹Ÿåˆæ¨¡å‹å¹¶è®¡ç®—æ®‹å·®
        model = LinearRegression()
        
        if self.Y.ndim == 1:
            model.fit(X_poly, self.Y)
            Y_pred = model.predict(X_poly)
            mse = mean_squared_error(self.Y, Y_pred)
        else:
            mses = []
            for i in range(self.Y.shape[1]):
                model.fit(X_poly, self.Y[:, i])
                Y_pred = model.predict(X_poly)
                mse = mean_squared_error(self.Y[:, i], Y_pred)
                mses.append(mse)
            mse = np.mean(mses)
        
        # è®¡ç®—AICå’ŒBIC
        log_likelihood = -n_samples * np.log(mse + 1e-8) / 2
        aic = 2 * n_features - 2 * log_likelihood
        bic = np.log(n_samples) * n_features - 2 * log_likelihood
        
        return aic, bic
    
    def _make_final_decision(self, theory_result, cv_result, ic_result):
        """ç»¼åˆå†³ç­–"""
        print("ğŸ¯ ç»¼åˆå†³ç­–...")
        
        # æ”¶é›†æ‰€æœ‰å»ºè®®
        suggestions = [
            theory_result['suggested_order'],
            cv_result['best_order'],
            ic_result['best_aic_order'],
            ic_result['best_bic_order']
        ]
        
        # è®¡ç®—åŠ æƒå¹³å‡ï¼ˆç†è®ºåˆ†ææƒé‡æ›´é«˜ï¼‰
        weights = [0.4, 0.3, 0.15, 0.15]
        weighted_order = sum(s * w for s, w in zip(suggestions, weights))
        
        # é€‰æ‹©æœ€æ¥è¿‘çš„æ•´æ•°é˜¶æ•°
        final_order = int(round(weighted_order))
        
        # ç¡®ä¿åœ¨åˆç†èŒƒå›´å†…
        final_order = max(1, min(final_order, self.max_order))
        
        print(f"   ç†è®ºå»ºè®®: {theory_result['suggested_order']}")
        print(f"   äº¤å‰éªŒè¯: {cv_result['best_order']}")
        print(f"   AICå»ºè®®: {ic_result['best_aic_order']}")
        print(f"   BICå»ºè®®: {ic_result['best_bic_order']}")
        print(f"   åŠ æƒå¹³å‡: {weighted_order:.2f}")
        print(f"   æœ€ç»ˆå†³ç­–: {final_order}")
        
        return final_order

def demonstrate_order_selection():
    """æ¼”ç¤ºé˜¶æ•°é€‰æ‹©è¿‡ç¨‹"""
    print("ğŸ¯ PCEæœ€ä¼˜é˜¶æ•°é€‰æ‹©æ¼”ç¤º")
    print("=" * 60)
    
    # ç”Ÿæˆä¸åŒå¤æ‚åº¦çš„æµ‹è¯•å‡½æ•°
    np.random.seed(42)
    X = np.random.uniform(-1, 1, (1000, 2))
    
    test_functions = {
        "çº¿æ€§å‡½æ•°": lambda x1, x2: 0.5 + 1.2*x1 + 0.8*x2,
        "äºŒæ¬¡å‡½æ•°": lambda x1, x2: 0.5 + 1.2*x1 + 0.8*x2 + 0.6*x1**2 + 0.4*x1*x2 + 0.3*x2**2,
        "ä¸‰æ¬¡å‡½æ•°": lambda x1, x2: 0.5 + x1 + x2 + x1**2 + x1*x2 + x2**2 + 0.3*x1**3 + 0.2*x1**2*x2,
        "å¤æ‚éçº¿æ€§": lambda x1, x2: np.sin(2*x1) * np.cos(2*x2) + 0.3*np.exp(-0.5*(x1**2 + x2**2))
    }
    
    results = {}
    
    for func_name, func in test_functions.items():
        print(f"\n{'='*20} {func_name} {'='*20}")
        
        # ç”Ÿæˆæ•°æ®
        Y = np.array([func(X[i, 0], X[i, 1]) for i in range(len(X))])
        Y = Y.reshape(-1, 1)
        
        # é€‰æ‹©æœ€ä¼˜é˜¶æ•°
        selector = PCEOrderSelector(X, Y, max_order=4)
        result = selector.select_optimal_order()
        
        results[func_name] = result
        
        print(f"âœ… {func_name}æœ€ä¼˜é˜¶æ•°: {result['optimal_order']}")
    
    return results

if __name__ == "__main__":
    results = demonstrate_order_selection()
    
    print("\n" + "="*60)
    print("ğŸ“Š æ€»ç»“æŠ¥å‘Š")
    print("="*60)
    
    for func_name, result in results.items():
        print(f"\n{func_name}:")
        print(f"  æœ€ä¼˜é˜¶æ•°: {result['optimal_order']}")
        print(f"  éçº¿æ€§å¼ºåº¦: {result['nonlinearity_metrics']['linear_correlation']['nonlinearity_ratio']:.3f}")
        print(f"  ç†è®ºå»ºè®®: {result['theory_based_order']['suggested_order']} ({result['theory_based_order']['reason']})")
