#!/usr/bin/env python3
"""
PCEè®­ç»ƒæ—¶é—´å’Œç²¾åº¦åˆ†æ
æµ‹è¯•ä¸åŒé…ç½®ä¸‹PCEçš„è®­ç»ƒæ—¶é—´å’Œç²¾åº¦è¡¨ç°
"""

import numpy as np
import matplotlib.pyplot as plt
import time
from pce_trainer import PCETrainer
import pandas as pd
import seaborn as sns

def test_training_time_vs_samples():
    """æµ‹è¯•è®­ç»ƒæ—¶é—´ä¸æ ·æœ¬æ•°é‡çš„å…³ç³»"""
    print("=" * 60)
    print("æµ‹è¯•ï¼šè®­ç»ƒæ—¶é—´ vs æ ·æœ¬æ•°é‡")
    print("=" * 60)
    
    sample_sizes = [500, 1000, 2000, 5000, 10000]
    training_times = []
    test_accuracies = []
    
    for n_samples in sample_sizes:
        print(f"\næµ‹è¯•æ ·æœ¬æ•°é‡: {n_samples}")
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = PCETrainer(input_dim=2, output_dim=78, polynomial_order=2)
        
        # ç”Ÿæˆæ•°æ®
        X, Y = trainer.generate_training_data(n_samples=n_samples, noise_level=0.01)
        
        # æµ‹é‡è®­ç»ƒæ—¶é—´
        start_time = time.time()
        results = trainer.train(X, Y, test_size=0.2)
        training_time = time.time() - start_time
        
        training_times.append(training_time)
        test_accuracies.append(results['test_r2'])
        
        print(f"  è®­ç»ƒæ—¶é—´: {training_time:.3f} ç§’")
        print(f"  æµ‹è¯•RÂ²: {results['test_r2']:.6f}")
    
    return sample_sizes, training_times, test_accuracies

def test_polynomial_order_effect():
    """æµ‹è¯•å¤šé¡¹å¼é˜¶æ•°å¯¹ç²¾åº¦å’Œè®­ç»ƒæ—¶é—´çš„å½±å“"""
    print("=" * 60)
    print("æµ‹è¯•ï¼šå¤šé¡¹å¼é˜¶æ•°çš„å½±å“")
    print("=" * 60)
    
    # æ³¨æ„ï¼šé«˜é˜¶å¤šé¡¹å¼éœ€è¦ä¿®æ”¹PCETrainerä»£ç æ”¯æŒ
    # è¿™é‡Œåªæµ‹è¯•2é˜¶ï¼Œå±•ç¤ºæ¦‚å¿µ
    
    results = []
    
    # å›ºå®šæ ·æœ¬æ•°é‡
    n_samples = 2000
    
    # æµ‹è¯•ä¸åŒçš„æ­£åˆ™åŒ–å‚æ•°ï¼ˆæ¨¡æ‹Ÿä¸åŒå¤æ‚åº¦ï¼‰
    regularizations = [1e-8, 1e-6, 1e-4, 1e-2]
    
    for reg in regularizations:
        print(f"\næµ‹è¯•æ­£åˆ™åŒ–å‚æ•°: {reg}")
        
        trainer = PCETrainer(input_dim=2, output_dim=78, polynomial_order=2)
        X, Y = trainer.generate_training_data(n_samples=n_samples, noise_level=0.01)
        
        start_time = time.time()
        train_results = trainer.train(X, Y, regularization=reg)
        training_time = time.time() - start_time
        
        results.append({
            'regularization': reg,
            'training_time': training_time,
            'test_r2': train_results['test_r2'],
            'test_mse': train_results['test_mse']
        })
        
        print(f"  è®­ç»ƒæ—¶é—´: {training_time:.3f} ç§’")
        print(f"  æµ‹è¯•RÂ²: {train_results['test_r2']:.6f}")
        print(f"  æµ‹è¯•MSE: {train_results['test_mse']:.6f}")
    
    return results

def compare_with_neural_network():
    """å¯¹æ¯”PCEå’Œç¥ç»ç½‘ç»œçš„è®­ç»ƒæ—¶é—´"""
    print("=" * 60)
    print("å¯¹æ¯”ï¼šPCE vs ç¥ç»ç½‘ç»œè®­ç»ƒæ—¶é—´")
    print("=" * 60)
    
    from sklearn.neural_network import MLPRegressor
    from sklearn.preprocessing import StandardScaler
    
    # å‡†å¤‡æ•°æ®
    trainer = PCETrainer(input_dim=2, output_dim=78, polynomial_order=2)
    X, Y = trainer.generate_training_data(n_samples=2000, noise_level=0.01)
    
    # PCEè®­ç»ƒ
    print("\nè®­ç»ƒPCE...")
    start_time = time.time()
    pce_results = trainer.train(X, Y)
    pce_time = time.time() - start_time
    
    # ç¥ç»ç½‘ç»œè®­ç»ƒ
    print("\nè®­ç»ƒç¥ç»ç½‘ç»œ...")
    scaler_X = StandardScaler()
    scaler_Y = StandardScaler()
    
    X_scaled = scaler_X.fit_transform(X)
    Y_scaled = scaler_Y.fit_transform(Y)
    
    nn_model = MLPRegressor(
        hidden_layer_sizes=(64, 32),
        max_iter=500,
        random_state=42,
        early_stopping=True,
        validation_fraction=0.2
    )
    
    start_time = time.time()
    nn_model.fit(X_scaled, Y_scaled)
    nn_time = time.time() - start_time
    
    print(f"\nPCEè®­ç»ƒæ—¶é—´: {pce_time:.3f} ç§’")
    print(f"ç¥ç»ç½‘ç»œè®­ç»ƒæ—¶é—´: {nn_time:.3f} ç§’")
    print(f"é€Ÿåº¦æå‡: {nn_time/pce_time:.1f}x")
    
    print(f"\nPCEæµ‹è¯•RÂ²: {pce_results['test_r2']:.6f}")
    
    return pce_time, nn_time, pce_results['test_r2']

def create_training_analysis_plots():
    """åˆ›å»ºè®­ç»ƒåˆ†æå›¾è¡¨"""
    
    # æµ‹è¯•1ï¼šæ ·æœ¬æ•°é‡ vs è®­ç»ƒæ—¶é—´
    sample_sizes, training_times, test_accuracies = test_training_time_vs_samples()
    
    # æµ‹è¯•2ï¼šæ­£åˆ™åŒ–å‚æ•°å½±å“
    reg_results = test_polynomial_order_effect()
    
    # æµ‹è¯•3ï¼šä¸ç¥ç»ç½‘ç»œå¯¹æ¯”
    pce_time, nn_time, pce_r2 = compare_with_neural_network()
    
    # åˆ›å»ºå›¾è¡¨
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('PCE Training Analysis', fontsize=16, fontweight='bold')
    
    # å›¾1ï¼šæ ·æœ¬æ•°é‡ vs è®­ç»ƒæ—¶é—´
    axes[0, 0].plot(sample_sizes, training_times, 'bo-', linewidth=2, markersize=8)
    axes[0, 0].set_xlabel('Number of Training Samples')
    axes[0, 0].set_ylabel('Training Time (seconds)')
    axes[0, 0].set_title('Training Time vs Sample Size')
    axes[0, 0].grid(True, alpha=0.3)
    
    # æ·»åŠ çº¿æ€§æ‹Ÿåˆ
    z = np.polyfit(sample_sizes, training_times, 1)
    p = np.poly1d(z)
    axes[0, 0].plot(sample_sizes, p(sample_sizes), "r--", alpha=0.8, 
                    label=f'Linear fit: {z[0]:.2e}x + {z[1]:.3f}')
    axes[0, 0].legend()
    
    # å›¾2ï¼šæ ·æœ¬æ•°é‡ vs ç²¾åº¦
    axes[0, 1].plot(sample_sizes, test_accuracies, 'go-', linewidth=2, markersize=8)
    axes[0, 1].set_xlabel('Number of Training Samples')
    axes[0, 1].set_ylabel('Test RÂ² Score')
    axes[0, 1].set_title('Accuracy vs Sample Size')
    axes[0, 1].grid(True, alpha=0.3)
    axes[0, 1].set_ylim([min(test_accuracies) - 0.01, max(test_accuracies) + 0.01])
    
    # å›¾3ï¼šæ­£åˆ™åŒ–å‚æ•°å½±å“
    reg_values = [r['regularization'] for r in reg_results]
    reg_r2_values = [r['test_r2'] for r in reg_results]
    reg_times = [r['training_time'] for r in reg_results]
    
    axes[1, 0].semilogx(reg_values, reg_r2_values, 'mo-', linewidth=2, markersize=8)
    axes[1, 0].set_xlabel('Regularization Parameter')
    axes[1, 0].set_ylabel('Test RÂ² Score')
    axes[1, 0].set_title('Regularization Effect on Accuracy')
    axes[1, 0].grid(True, alpha=0.3)
    
    # å›¾4ï¼šPCE vs NNè®­ç»ƒæ—¶é—´å¯¹æ¯”
    methods = ['PCE', 'Neural Network']
    times = [pce_time, nn_time]
    colors = ['#2E86AB', '#A23B72']
    
    bars = axes[1, 1].bar(methods, times, color=colors, alpha=0.7)
    axes[1, 1].set_ylabel('Training Time (seconds)')
    axes[1, 1].set_title('PCE vs Neural Network Training Time')
    axes[1, 1].grid(True, alpha=0.3, axis='y')
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, time_val in zip(bars, times):
        height = bar.get_height()
        axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                        f'{time_val:.3f}s', ha='center', va='bottom', fontweight='bold')
    
    # æ·»åŠ é€Ÿåº¦æå‡æ ‡æ³¨
    axes[1, 1].text(0.5, max(times) * 0.8, f'PCE is {nn_time/pce_time:.1f}x faster', 
                    ha='center', va='center', fontsize=12, fontweight='bold',
                    bbox=dict(boxstyle="round,pad=0.3", facecolor="yellow", alpha=0.7))
    
    plt.tight_layout()
    plt.savefig('pce_training_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return fig

def print_training_insights():
    """æ‰“å°è®­ç»ƒæ´å¯Ÿ"""
    print("\n" + "=" * 60)
    print("PCEè®­ç»ƒç‰¹ç‚¹æ€»ç»“")
    print("=" * 60)
    
    print("\nğŸš€ è®­ç»ƒé€Ÿåº¦ç‰¹ç‚¹ï¼š")
    print("â€¢ PCEè®­ç»ƒæ˜¯ä¸€æ¬¡æ€§è§£ææ±‚è§£ï¼Œä¸æ˜¯è¿­ä»£è¿‡ç¨‹")
    print("â€¢ è®­ç»ƒæ—¶é—´ä¸æ ·æœ¬æ•°é‡å‘ˆçº¿æ€§å…³ç³»")
    print("â€¢ å…¸å‹è®­ç»ƒæ—¶é—´ï¼š2000æ ·æœ¬ < 1ç§’")
    print("â€¢ æ¯”ç¥ç»ç½‘ç»œå¿«10-100å€")
    
    print("\nğŸ¯ ç²¾åº¦æå‡æ–¹æ³•ï¼š")
    print("â€¢ å¢åŠ è®­ç»ƒæ ·æœ¬æ•°é‡ï¼ˆè¾¹é™…æ•ˆåº”é€’å‡ï¼‰")
    print("â€¢ æé«˜å¤šé¡¹å¼é˜¶æ•°ï¼ˆéœ€è¦ä¿®æ”¹ä»£ç ï¼‰")
    print("â€¢ è°ƒæ•´æ­£åˆ™åŒ–å‚æ•°")
    print("â€¢ æ”¹å–„æ•°æ®è´¨é‡ï¼ˆå‡å°‘å™ªå£°ï¼‰")
    
    print("\nâŒ æ— æ•ˆçš„ç²¾åº¦æå‡æ–¹æ³•ï¼š")
    print("â€¢ ç»§ç»­è®­ç»ƒï¼ˆPCEæ˜¯è§£æè§£ï¼Œä¸€æ¬¡ç¡®å®šï¼‰")
    print("â€¢ è°ƒæ•´å­¦ä¹ ç‡ï¼ˆPCEæ²¡æœ‰å­¦ä¹ ç‡æ¦‚å¿µï¼‰")
    print("â€¢ æ—©åœç­–ç•¥ï¼ˆPCEä¸éœ€è¦æ—©åœï¼‰")
    print("â€¢ æ‰¹æ¬¡å¤§å°ï¼ˆPCEä¸€æ¬¡å¤„ç†æ‰€æœ‰æ•°æ®ï¼‰")
    
    print("\nğŸ’¡ å…³é”®æ´å¯Ÿï¼š")
    print("â€¢ PCEçš„ç²¾åº¦ä¸Šé™ç”±å¤šé¡¹å¼é˜¶æ•°å†³å®š")
    print("â€¢ å¯¹äºå¤æ‚éçº¿æ€§å‡½æ•°ï¼ŒPCEç²¾åº¦æœ‰å¤©ç„¶é™åˆ¶")
    print("â€¢ PCEçš„ä¼˜åŠ¿åœ¨äºé€Ÿåº¦ï¼Œä¸åœ¨äºç²¾åº¦")
    print("â€¢ é€‰æ‹©PCEæ—¶è¦æƒè¡¡ç²¾åº¦ä¸é€Ÿåº¦çš„éœ€æ±‚")

def main():
    """ä¸»å‡½æ•°"""
    print("PCE Training Time and Accuracy Analysis")
    print("=" * 60)
    
    # è¿è¡Œåˆ†æ
    create_training_analysis_plots()
    
    # æ‰“å°æ´å¯Ÿ
    print_training_insights()
    
    print(f"\nåˆ†æå®Œæˆï¼ç”Ÿæˆæ–‡ä»¶ï¼špce_training_analysis.png")

if __name__ == "__main__":
    main()
