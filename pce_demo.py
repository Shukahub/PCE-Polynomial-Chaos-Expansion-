#!/usr/bin/env python3
"""
PCE vs Neural Network æ€§èƒ½å¯¹æ¯”æ¼”ç¤º
å±•ç¤ºPCEåœ¨æ¨ç†é€Ÿåº¦å’Œç²¾åº¦æ–¹é¢ç›¸å¯¹äºç¥ç»ç½‘ç»œçš„ä¼˜åŠ¿
"""

import numpy as np
import time
import matplotlib.pyplot as plt
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

from pce_trainer import PCETrainer
from data_generator import DataGenerator

class PerformanceComparator:
    def __init__(self):
        self.pce_model = None
        self.nn_model = None
        self.input_scaler = StandardScaler()
        self.output_scaler = StandardScaler()
    
    def prepare_data(self, n_samples=2000):
        """å‡†å¤‡è®­ç»ƒå’Œæµ‹è¯•æ•°æ®"""
        print("Preparing training data...")
        
        generator = DataGenerator(input_dim=2, output_dim=78)
        X, Y = generator.generate_mixed_data(n_samples=n_samples, noise_level=0.02)
        
        # åˆ†å‰²æ•°æ®
        X_train, X_test, Y_train, Y_test = train_test_split(
            X, Y, test_size=0.2, random_state=42
        )
        
        return X_train, X_test, Y_train, Y_test
    
    def train_pce(self, X_train, Y_train, auto_order_selection=True, polynomial_order=None):
        """è®­ç»ƒPCEæ¨¡å‹ï¼ˆæ”¯æŒæ™ºèƒ½é˜¶æ•°é€‰æ‹©ï¼‰"""
        print("\n" + "="*60)
        print("ğŸš€ PCEæ¨¡å‹è®­ç»ƒ")
        print("="*60)

        start_time = time.time()

        # åˆ›å»ºPCEè®­ç»ƒå™¨ï¼ˆæ”¯æŒè‡ªåŠ¨é˜¶æ•°é€‰æ‹©ï¼‰
        self.pce_model = PCETrainer(
            input_dim=2,
            output_dim=78,
            polynomial_order=polynomial_order,
            auto_order_selection=auto_order_selection
        )

        # è®­ç»ƒæ¨¡å‹
        results = self.pce_model.train(X_train, Y_train, test_size=0.1, max_order=4)

        pce_train_time = time.time() - start_time
        print(f"\nâ±ï¸  PCEæ€»è®­ç»ƒæ—¶é—´: {pce_train_time:.2f} ç§’")

        # æ˜¾ç¤ºé˜¶æ•°é€‰æ‹©ç»“æœ
        if self.pce_model.order_selection_results:
            self._display_order_selection_results()

        return pce_train_time

    def _display_order_selection_results(self):
        """æ˜¾ç¤ºé˜¶æ•°é€‰æ‹©ç»“æœ"""
        results = self.pce_model.order_selection_results

        print("\n" + "="*50)
        print("ğŸ“Š æ™ºèƒ½é˜¶æ•°é€‰æ‹©ç»“æœ")
        print("="*50)

        print(f"ğŸ¯ æœ€ä¼˜é˜¶æ•°: {results['optimal_order']}")
        print(f"ğŸ“ˆ éçº¿æ€§å¼ºåº¦åˆ†æ•°: {results['theory_order']['nonlinearity_score']:.3f}")
        print(f"ğŸ’¡ ç†è®ºå»ºè®®: {results['theory_order']['suggested_order']} ({results['theory_order']['reason']})")
        print(f"ğŸ”„ äº¤å‰éªŒè¯æœ€ä½³: {results['cv_order']['best_order']}")
        print(f"ğŸ“Š AICå»ºè®®: {results['ic_order']['best_aic_order']}")
        print(f"ğŸ“Š BICå»ºè®®: {results['ic_order']['best_bic_order']}")

        # æ˜¾ç¤ºéçº¿æ€§åˆ†æè¯¦æƒ…
        metrics = results['nonlinearity_metrics']
        print(f"\nğŸ” éçº¿æ€§å¼ºåº¦åˆ†æ:")
        print(f"   çº¿æ€§ç›¸å…³æ€§: {metrics['linear_correlation']['avg_linear_corr']:.3f}")
        print(f"   éçº¿æ€§æ¯”ä¾‹: {metrics['linear_correlation']['nonlinearity_ratio']:.3f}")
        print(f"   é«˜é˜¶çŸ©å¤æ‚åº¦: {metrics['higher_moments']['moment_complexity']:.3f}")
        print(f"   é«˜é¢‘æˆåˆ†æ¯”ä¾‹: {metrics['frequency_analysis']['avg_high_freq_ratio']:.3f}")
        print(f"   å±€éƒ¨çº¿æ€§åº¦: {metrics['local_linearity']['avg_local_linearity']:.3f}")
        print(f"   æ¢¯åº¦å˜åŒ–: {metrics['gradient_variation']['avg_gradient_variation']:.3f}")

        print("="*50)
    
    def train_neural_network(self, X_train, Y_train):
        """è®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹"""
        print("\nTraining Neural Network model...")
        start_time = time.time()
        
        # æ•°æ®æ ‡å‡†åŒ–
        X_train_scaled = self.input_scaler.fit_transform(X_train)
        Y_train_scaled = self.output_scaler.fit_transform(Y_train)
        
        # åˆ›å»ºç¥ç»ç½‘ç»œï¼ˆç±»ä¼¼å¤æ‚åº¦çš„ç½‘ç»œï¼‰
        self.nn_model = MLPRegressor(
            hidden_layer_sizes=(100, 50),  # ä¸¤ä¸ªéšè—å±‚
            activation='relu',
            solver='adam',
            alpha=0.001,
            batch_size='auto',
            learning_rate='constant',
            learning_rate_init=0.001,
            max_iter=500,
            random_state=42,
            early_stopping=True,
            validation_fraction=0.1,
            n_iter_no_change=20
        )
        
        self.nn_model.fit(X_train_scaled, Y_train_scaled)
        
        nn_train_time = time.time() - start_time
        print(f"Neural Network training time: {nn_train_time:.2f} seconds")
        
        return nn_train_time
    
    def benchmark_inference_speed(self, X_test, n_iterations=1000):
        """åŸºå‡†æµ‹è¯•æ¨ç†é€Ÿåº¦"""
        print(f"\nBenchmarking inference speed ({n_iterations} iterations)...")
        
        # PCEæ¨ç†é€Ÿåº¦æµ‹è¯•
        start_time = time.time()
        for _ in range(n_iterations):
            _ = self.pce_model.predict(X_test)
        pce_inference_time = time.time() - start_time
        
        # ç¥ç»ç½‘ç»œæ¨ç†é€Ÿåº¦æµ‹è¯•
        X_test_scaled = self.input_scaler.transform(X_test)
        start_time = time.time()
        for _ in range(n_iterations):
            Y_pred_scaled = self.nn_model.predict(X_test_scaled)
            _ = self.output_scaler.inverse_transform(Y_pred_scaled)
        nn_inference_time = time.time() - start_time
        
        print(f"PCE inference time: {pce_inference_time:.4f} seconds")
        print(f"NN inference time: {nn_inference_time:.4f} seconds")
        print(f"Speedup: {nn_inference_time/pce_inference_time:.2f}x")
        
        return pce_inference_time, nn_inference_time
    
    def evaluate_accuracy(self, X_test, Y_test):
        """è¯„ä¼°æ¨¡å‹ç²¾åº¦"""
        print("\nEvaluating model accuracy...")
        
        # PCEé¢„æµ‹
        Y_pred_pce = self.pce_model.predict(X_test)
        pce_mse = mean_squared_error(Y_test, Y_pred_pce)
        pce_r2 = r2_score(Y_test, Y_pred_pce)
        
        # ç¥ç»ç½‘ç»œé¢„æµ‹
        X_test_scaled = self.input_scaler.transform(X_test)
        Y_pred_nn_scaled = self.nn_model.predict(X_test_scaled)
        Y_pred_nn = self.output_scaler.inverse_transform(Y_pred_nn_scaled)
        nn_mse = mean_squared_error(Y_test, Y_pred_nn)
        nn_r2 = r2_score(Y_test, Y_pred_nn)
        
        print(f"PCE - MSE: {pce_mse:.6f}, RÂ²: {pce_r2:.6f}")
        print(f"NN  - MSE: {nn_mse:.6f}, RÂ²: {nn_r2:.6f}")
        
        return {
            'pce_mse': pce_mse, 'pce_r2': pce_r2, 'Y_pred_pce': Y_pred_pce,
            'nn_mse': nn_mse, 'nn_r2': nn_r2, 'Y_pred_nn': Y_pred_nn
        }
    
    def visualize_comparison(self, Y_test, results):
        """å¯è§†åŒ–å¯¹æ¯”ç»“æœ"""
        print("\nGenerating comparison visualizations...")
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        # å‰3ä¸ªè¾“å‡ºç»´åº¦çš„é¢„æµ‹å¯¹æ¯”
        for i in range(3):
            # PCEé¢„æµ‹
            axes[0, i].scatter(Y_test[:, i], results['Y_pred_pce'][:, i], 
                              alpha=0.6, color='blue', label='PCE')
            axes[0, i].plot([Y_test[:, i].min(), Y_test[:, i].max()], 
                           [Y_test[:, i].min(), Y_test[:, i].max()], 'r--')
            axes[0, i].set_xlabel(f'True Output {i+1}')
            axes[0, i].set_ylabel(f'Predicted Output {i+1}')
            axes[0, i].set_title(f'PCE: Output {i+1}')
            axes[0, i].grid(True, alpha=0.3)
            axes[0, i].legend()
            
            # ç¥ç»ç½‘ç»œé¢„æµ‹
            axes[1, i].scatter(Y_test[:, i], results['Y_pred_nn'][:, i], 
                              alpha=0.6, color='green', label='Neural Network')
            axes[1, i].plot([Y_test[:, i].min(), Y_test[:, i].max()], 
                           [Y_test[:, i].min(), Y_test[:, i].max()], 'r--')
            axes[1, i].set_xlabel(f'True Output {i+1}')
            axes[1, i].set_ylabel(f'Predicted Output {i+1}')
            axes[1, i].set_title(f'Neural Network: Output {i+1}')
            axes[1, i].grid(True, alpha=0.3)
            axes[1, i].legend()
        
        plt.tight_layout()
        plt.savefig('pce_vs_nn_comparison.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def generate_performance_report(self, train_times, inference_times, accuracy_results):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        pce_train_time, nn_train_time = train_times
        pce_inference_time, nn_inference_time = inference_times
        
        report = f"""
{'='*60}
PCE vs Neural Network Performance Report
{'='*60}

Training Performance:
  PCE Training Time:        {pce_train_time:.2f} seconds
  NN Training Time:         {nn_train_time:.2f} seconds
  Training Speedup:         {nn_train_time/pce_train_time:.2f}x (PCE faster)

Inference Performance:
  PCE Inference Time:       {pce_inference_time:.4f} seconds
  NN Inference Time:        {nn_inference_time:.4f} seconds
  Inference Speedup:        {nn_inference_time/pce_inference_time:.2f}x (PCE faster)

Accuracy Comparison:
  PCE MSE:                  {accuracy_results['pce_mse']:.6f}
  NN MSE:                   {accuracy_results['nn_mse']:.6f}
  MSE Ratio (NN/PCE):       {accuracy_results['nn_mse']/accuracy_results['pce_mse']:.2f}
  
  PCE RÂ²:                   {accuracy_results['pce_r2']:.6f}
  NN RÂ²:                    {accuracy_results['nn_r2']:.6f}
  RÂ² Difference:            {accuracy_results['nn_r2'] - accuracy_results['pce_r2']:.6f}

Summary:
  PCE provides {nn_inference_time/pce_inference_time:.1f}x faster inference with 
  {'better' if accuracy_results['pce_mse'] < accuracy_results['nn_mse'] else 'comparable'} accuracy.
  
  PCE is particularly suitable for:
  - Real-time applications requiring fast inference
  - Embedded systems with limited computational resources
  - Applications where model interpretability is important
  - Scenarios with smooth, polynomial-like relationships

{'='*60}
"""
        
        print(report)
        
        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        with open('performance_report.txt', 'w', encoding='utf-8') as f:
            f.write(report)
        
        print("Performance report saved to performance_report.txt")

def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œå®Œæ•´çš„æ™ºèƒ½PCEæ€§èƒ½å¯¹æ¯”"""
    print("=" * 80)
    print("ğŸ¯ PCE vs Neural Network æ™ºèƒ½æ€§èƒ½å¯¹æ¯”ç³»ç»Ÿ")
    print("=" * 80)

    comparator = PerformanceComparator()

    # 1. å‡†å¤‡æ•°æ®
    print("\nğŸ“Š å‡†å¤‡è®­ç»ƒæ•°æ®...")
    X_train, X_test, Y_train, Y_test = comparator.prepare_data(n_samples=2000)
    print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ: {X_train.shape[0]} è®­ç»ƒæ ·æœ¬, {X_test.shape[0]} æµ‹è¯•æ ·æœ¬")

    # 2. æ™ºèƒ½PCEè®­ç»ƒï¼ˆè‡ªåŠ¨é˜¶æ•°é€‰æ‹©ï¼‰
    print("\nğŸ§  å¼€å§‹æ™ºèƒ½PCEè®­ç»ƒ...")
    pce_train_time = comparator.train_pce(X_train, Y_train, auto_order_selection=True)

    # 3. ç¥ç»ç½‘ç»œè®­ç»ƒ
    print("\nğŸ¤– å¼€å§‹ç¥ç»ç½‘ç»œè®­ç»ƒ...")
    nn_train_time = comparator.train_neural_network(X_train, Y_train)
    
    # 3. åŸºå‡†æµ‹è¯•æ¨ç†é€Ÿåº¦
    pce_inference_time, nn_inference_time = comparator.benchmark_inference_speed(X_test, n_iterations=1000)
    
    # 4. è¯„ä¼°ç²¾åº¦
    accuracy_results = comparator.evaluate_accuracy(X_test, Y_test)
    
    # 5. å¯è§†åŒ–å¯¹æ¯”
    comparator.visualize_comparison(Y_test, accuracy_results)
    
    # 6. ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
    comparator.generate_performance_report(
        (pce_train_time, nn_train_time),
        (pce_inference_time, nn_inference_time),
        accuracy_results
    )
    
    # 7. ä¿å­˜PCEæ¨¡å‹ç”¨äºFortran
    comparator.pce_model.save_model('final_pce_model.pkl')
    comparator.pce_model.export_fortran_coefficients('final_pce_coefficients.txt')
    
    print("\nDemo completed successfully!")
    print("Files generated:")
    print("  - final_pce_model.pkl: è®­ç»ƒå¥½çš„PCEæ¨¡å‹")
    print("  - final_pce_coefficients.txt: Fortranç³»æ•°æ–‡ä»¶")
    print("  - pce_vs_nn_comparison.png: å¯¹æ¯”å¯è§†åŒ–")
    print("  - performance_report.txt: æ€§èƒ½æŠ¥å‘Š")

if __name__ == "__main__":
    main()
