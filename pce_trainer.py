#!/usr/bin/env python3
"""
PCE (Polynomial Chaos Expansion) Trainer with Intelligent Order Selection
ç”¨äºè®­ç»ƒPCEæ¨¡å‹æ›¿ä»£ç¥ç»ç½‘ç»œçš„Pythonç¨‹åº

PCEä½¿ç”¨å¤šé¡¹å¼åŸºå‡½æ•°æ¥è¿‘ä¼¼å¤æ‚çš„è¾“å…¥-è¾“å‡ºå…³ç³»
é›†æˆäº†æ™ºèƒ½é˜¶æ•°é€‰æ‹©åŠŸèƒ½ï¼Œå¯ä»¥è‡ªåŠ¨åˆ†æå‡½æ•°éçº¿æ€§å¼ºåº¦å¹¶æ¨èæœ€ä¼˜é˜¶æ•°
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import Ridge, LinearRegression
from sklearn.neighbors import NearestNeighbors
from scipy import stats
from scipy.fft import fft, fftfreq
import pickle
import os
import warnings
warnings.filterwarnings('ignore')

class PCETrainer:
    def __init__(self, input_dim=2, output_dim=78, polynomial_order=None, auto_order_selection=True):
        """
        åˆå§‹åŒ–PCEè®­ç»ƒå™¨

        Args:
            input_dim: è¾“å…¥ç»´åº¦
            output_dim: è¾“å‡ºç»´åº¦
            polynomial_order: å¤šé¡¹å¼é˜¶æ•° (Noneè¡¨ç¤ºè‡ªåŠ¨é€‰æ‹©)
            auto_order_selection: æ˜¯å¦å¯ç”¨è‡ªåŠ¨é˜¶æ•°é€‰æ‹©
        """
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.polynomial_order = polynomial_order
        self.auto_order_selection = auto_order_selection

        # å¦‚æœæ²¡æœ‰æŒ‡å®šé˜¶æ•°ä¸”å¯ç”¨è‡ªåŠ¨é€‰æ‹©ï¼Œåˆ™è®¾ä¸ºNoneç­‰å¾…åç»­ç¡®å®š
        if self.polynomial_order is None and not auto_order_selection:
            self.polynomial_order = 2  # é»˜è®¤2é˜¶

        # è®¡ç®—å¤šé¡¹å¼åŸºå‡½æ•°çš„æ•°é‡ï¼ˆå¦‚æœé˜¶æ•°å·²ç¡®å®šï¼‰
        if self.polynomial_order is not None:
            self.n_basis = self._calculate_basis_count()
        else:
            self.n_basis = None

        # PCEç³»æ•°çŸ©é˜µ (output_dim x n_basis)
        self.coefficients = None

        # æ•°æ®æ ‡å‡†åŒ–å™¨
        self.input_scaler = StandardScaler()
        self.output_scaler = StandardScaler()

        # é˜¶æ•°é€‰æ‹©ç›¸å…³
        self.order_selection_results = None

        print(f"PCE Trainer initialized:")
        print(f"  Input dimension: {self.input_dim}")
        print(f"  Output dimension: {self.output_dim}")
        if self.polynomial_order is not None:
            print(f"  Polynomial order: {self.polynomial_order}")
            print(f"  Number of basis functions: {self.n_basis}")
        else:
            print(f"  Polynomial order: Auto-selection enabled")
        print(f"  Auto order selection: {self.auto_order_selection}")
    
    def _calculate_basis_count(self):
        """è®¡ç®—å¤šé¡¹å¼åŸºå‡½æ•°æ•°é‡"""
        if self.polynomial_order is None:
            return None

        # é€šç”¨å…¬å¼: C(n+d, d) where n=input_dim, d=polynomial_order
        from math import comb
        return comb(self.input_dim + self.polynomial_order, self.polynomial_order)

    def analyze_nonlinearity(self, X, Y):
        """åˆ†æå‡½æ•°éçº¿æ€§å¼ºåº¦"""
        print("ğŸ” åˆ†æå‡½æ•°éçº¿æ€§å¼ºåº¦...")

        # æ ‡å‡†åŒ–æ•°æ®
        scaler_X = StandardScaler()
        scaler_Y = StandardScaler()
        X_scaled = scaler_X.fit_transform(X)
        Y_scaled = scaler_Y.fit_transform(Y.reshape(-1, 1) if Y.ndim == 1 else Y)

        metrics = {}

        # 1. çº¿æ€§ç›¸å…³æ€§åˆ†æ
        metrics['linear_correlation'] = self._analyze_linear_correlation(X_scaled, Y_scaled)

        # 2. é«˜é˜¶çŸ©åˆ†æ
        metrics['higher_moments'] = self._analyze_higher_moments(Y_scaled)

        # 3. é¢‘åŸŸåˆ†æ
        metrics['frequency_analysis'] = self._analyze_frequency_domain(Y_scaled)

        # 4. å±€éƒ¨çº¿æ€§åº¦åˆ†æ
        metrics['local_linearity'] = self._analyze_local_linearity(X_scaled, Y_scaled)

        # 5. æ¢¯åº¦å˜åŒ–åˆ†æ
        metrics['gradient_variation'] = self._analyze_gradient_variation(X_scaled, Y_scaled)

        return metrics

    def _analyze_linear_correlation(self, X_scaled, Y_scaled):
        """çº¿æ€§ç›¸å…³æ€§åˆ†æ"""
        n_outputs = Y_scaled.shape[1] if Y_scaled.ndim > 1 else 1
        correlations = []

        for i in range(n_outputs):
            y = Y_scaled[:, i] if Y_scaled.ndim > 1 else Y_scaled

            # è®¡ç®—ä¸æ¯ä¸ªè¾“å…¥çš„çº¿æ€§ç›¸å…³æ€§
            corr_with_inputs = []
            for j in range(self.input_dim):
                corr = np.corrcoef(X_scaled[:, j], y)[0, 1]
                if not np.isnan(corr):
                    corr_with_inputs.append(abs(corr))

            # è®¡ç®—ä¸è¾“å…¥ä¹˜ç§¯é¡¹çš„ç›¸å…³æ€§
            interaction_corrs = []
            for j in range(self.input_dim):
                for k in range(j+1, self.input_dim):
                    interaction = X_scaled[:, j] * X_scaled[:, k]
                    corr = np.corrcoef(interaction, y)[0, 1]
                    if not np.isnan(corr):
                        interaction_corrs.append(abs(corr))

            # è®¡ç®—ä¸å¹³æ–¹é¡¹çš„ç›¸å…³æ€§
            quadratic_corrs = []
            for j in range(self.input_dim):
                quadratic = X_scaled[:, j] ** 2
                corr = np.corrcoef(quadratic, y)[0, 1]
                if not np.isnan(corr):
                    quadratic_corrs.append(abs(corr))

            correlations.append({
                'linear': np.mean(corr_with_inputs) if corr_with_inputs else 0,
                'interaction': np.mean(interaction_corrs) if interaction_corrs else 0,
                'quadratic': np.mean(quadratic_corrs) if quadratic_corrs else 0
            })

        avg_linear = np.mean([c['linear'] for c in correlations])
        avg_interaction = np.mean([c['interaction'] for c in correlations])
        avg_quadratic = np.mean([c['quadratic'] for c in correlations])

        return {
            'avg_linear_corr': avg_linear,
            'avg_interaction_corr': avg_interaction,
            'avg_quadratic_corr': avg_quadratic,
            'nonlinearity_ratio': (avg_quadratic + avg_interaction) / (avg_linear + 1e-8)
        }

    def _analyze_higher_moments(self, Y_scaled):
        """é«˜é˜¶çŸ©åˆ†æ"""
        n_outputs = Y_scaled.shape[1] if Y_scaled.ndim > 1 else 1
        skewness_values = []
        kurtosis_values = []

        for i in range(n_outputs):
            y = Y_scaled[:, i] if Y_scaled.ndim > 1 else Y_scaled
            skewness = abs(stats.skew(y))
            kurtosis = abs(stats.kurtosis(y))
            skewness_values.append(skewness)
            kurtosis_values.append(kurtosis)

        return {
            'avg_skewness': np.mean(skewness_values),
            'avg_kurtosis': np.mean(kurtosis_values),
            'moment_complexity': np.mean(skewness_values) + np.mean(kurtosis_values)
        }

    def _analyze_frequency_domain(self, Y_scaled):
        """é¢‘åŸŸåˆ†æ"""
        n_outputs = Y_scaled.shape[1] if Y_scaled.ndim > 1 else 1
        high_freq_ratios = []

        for i in range(n_outputs):
            y = Y_scaled[:, i] if Y_scaled.ndim > 1 else Y_scaled

            # FFTåˆ†æ
            fft_y = fft(y)
            freqs = fftfreq(len(y))

            # è®¡ç®—é«˜é¢‘æˆåˆ†æ¯”ä¾‹
            power_spectrum = np.abs(fft_y) ** 2
            total_power = np.sum(power_spectrum)

            # å®šä¹‰é«˜é¢‘ä¸ºé¢‘ç‡ > 0.1 (å½’ä¸€åŒ–é¢‘ç‡)
            high_freq_mask = np.abs(freqs) > 0.1
            high_freq_power = np.sum(power_spectrum[high_freq_mask])

            high_freq_ratio = high_freq_power / (total_power + 1e-8)
            high_freq_ratios.append(high_freq_ratio)

        return {
            'avg_high_freq_ratio': np.mean(high_freq_ratios),
            'max_high_freq_ratio': np.max(high_freq_ratios)
        }

    def _analyze_local_linearity(self, X_scaled, Y_scaled):
        """å±€éƒ¨çº¿æ€§åº¦åˆ†æ"""
        n_samples = X_scaled.shape[0]
        k = min(10, n_samples // 4)  # é€‰æ‹©åˆé€‚çš„kå€¼

        try:
            nbrs = NearestNeighbors(n_neighbors=k).fit(X_scaled)
            local_linearities = []

            # é‡‡æ ·åˆ†æ
            sample_indices = np.random.choice(n_samples, min(50, n_samples), replace=False)

            for i in sample_indices:
                # æ‰¾åˆ°kä¸ªæœ€è¿‘é‚»
                distances, indices = nbrs.kneighbors([X_scaled[i]])

                # åœ¨å±€éƒ¨åŒºåŸŸæ‹Ÿåˆçº¿æ€§æ¨¡å‹
                local_X = X_scaled[indices[0]]
                local_Y = Y_scaled[indices[0]] if Y_scaled.ndim > 1 else Y_scaled[indices[0]]

                if local_Y.ndim == 1:
                    local_Y = local_Y.reshape(-1, 1)

                local_r2_scores = []
                for j in range(local_Y.shape[1]):
                    try:
                        lr = LinearRegression()
                        lr.fit(local_X, local_Y[:, j])
                        local_pred = lr.predict(local_X)
                        local_r2 = r2_score(local_Y[:, j], local_pred)
                        local_r2_scores.append(max(0, local_r2))
                    except:
                        local_r2_scores.append(0)

                local_linearities.append(np.mean(local_r2_scores))

            return {
                'avg_local_linearity': np.mean(local_linearities),
                'min_local_linearity': np.min(local_linearities)
            }
        except:
            return {
                'avg_local_linearity': 0.5,
                'min_local_linearity': 0.5
            }

    def _analyze_gradient_variation(self, X_scaled, Y_scaled):
        """æ¢¯åº¦å˜åŒ–åˆ†æ"""
        n_outputs = Y_scaled.shape[1] if Y_scaled.ndim > 1 else 1
        gradients = []

        for i in range(n_outputs):
            y = Y_scaled[:, i] if Y_scaled.ndim > 1 else Y_scaled

            grad_variations = []
            for j in range(self.input_dim):
                try:
                    # æ’åºä»¥è®¡ç®—æ¢¯åº¦
                    sorted_indices = np.argsort(X_scaled[:, j])
                    sorted_x = X_scaled[sorted_indices, j]
                    sorted_y = y[sorted_indices]

                    # è®¡ç®—æ•°å€¼æ¢¯åº¦
                    grad = np.gradient(sorted_y, sorted_x)

                    # è®¡ç®—æ¢¯åº¦çš„å˜åŒ–ç¨‹åº¦
                    grad_variation = np.std(grad) / (np.mean(np.abs(grad)) + 1e-8)
                    grad_variations.append(grad_variation)
                except:
                    grad_variations.append(1.0)

            gradients.append(np.mean(grad_variations))

        return {
            'avg_gradient_variation': np.mean(gradients),
            'max_gradient_variation': np.max(gradients)
        }

    def select_optimal_order(self, X, Y, max_order=5):
        """æ™ºèƒ½é€‰æ‹©æœ€ä¼˜PCEé˜¶æ•°"""
        print("ğŸ¯ æ™ºèƒ½PCEé˜¶æ•°é€‰æ‹©")
        print("=" * 50)

        # 1. éçº¿æ€§å¼ºåº¦åˆ†æ
        nonlinearity_metrics = self.analyze_nonlinearity(X, Y)

        # 2. åŸºäºç†è®ºçš„é˜¶æ•°å»ºè®®
        theory_order = self._theory_based_order_selection(nonlinearity_metrics)

        # 3. äº¤å‰éªŒè¯é€‰æ‹©
        cv_order = self._cross_validation_order_selection(X, Y, max_order)

        # 4. ä¿¡æ¯å‡†åˆ™é€‰æ‹©
        ic_order = self._information_criterion_selection(X, Y, max_order)

        # 5. ç»¼åˆå†³ç­–
        optimal_order = self._make_final_decision(theory_order, cv_order, ic_order)

        # ä¿å­˜é€‰æ‹©ç»“æœ
        self.order_selection_results = {
            'optimal_order': optimal_order,
            'nonlinearity_metrics': nonlinearity_metrics,
            'theory_order': theory_order,
            'cv_order': cv_order,
            'ic_order': ic_order
        }

        # æ›´æ–°PCEé…ç½®
        self.polynomial_order = optimal_order
        self.n_basis = self._calculate_basis_count()

        print(f"âœ… æœ€ä¼˜é˜¶æ•°é€‰æ‹©å®Œæˆ: {optimal_order}")
        print(f"   åŸºå‡½æ•°æ•°é‡: {self.n_basis}")

        return optimal_order

    def _theory_based_order_selection(self, metrics):
        """åŸºäºç†è®ºçš„é˜¶æ•°é€‰æ‹©"""
        print("ğŸ“Š åŸºäºéçº¿æ€§å¼ºåº¦ç†è®ºåˆ†æ...")

        # è®¡ç®—ç»¼åˆéçº¿æ€§å¼ºåº¦åˆ†æ•°
        linear_corr = metrics['linear_correlation']['avg_linear_corr']
        nonlin_ratio = metrics['linear_correlation']['nonlinearity_ratio']
        moment_complexity = metrics['higher_moments']['moment_complexity']
        high_freq_ratio = metrics['frequency_analysis']['avg_high_freq_ratio']
        local_linearity = metrics['local_linearity']['avg_local_linearity']
        grad_variation = metrics['gradient_variation']['avg_gradient_variation']

        # å½’ä¸€åŒ–å¹¶è®¡ç®—ç»¼åˆåˆ†æ•°
        nonlinearity_score = (
            (1 - linear_corr) * 0.2 +
            min(nonlin_ratio, 2.0) * 0.2 +
            min(moment_complexity, 5.0) * 0.15 +
            min(high_freq_ratio, 1.0) * 0.2 +
            (1 - local_linearity) * 0.15 +
            min(grad_variation, 3.0) * 0.1
        )

        # åŸºäºåˆ†æ•°å»ºè®®é˜¶æ•°
        if nonlinearity_score < 0.3:
            suggested_order = 1
            reason = "å‡½æ•°æ¥è¿‘çº¿æ€§"
        elif nonlinearity_score < 0.6:
            suggested_order = 2
            reason = "ä¸­ç­‰éçº¿æ€§å¼ºåº¦"
        elif nonlinearity_score < 1.0:
            suggested_order = 3
            reason = "è¾ƒå¼ºéçº¿æ€§"
        elif nonlinearity_score < 1.5:
            suggested_order = 4
            reason = "å¼ºéçº¿æ€§"
        else:
            suggested_order = 5
            reason = "æå¼ºéçº¿æ€§"

        print(f"   éçº¿æ€§å¼ºåº¦åˆ†æ•°: {nonlinearity_score:.3f}")
        print(f"   ç†è®ºå»ºè®®é˜¶æ•°: {suggested_order} ({reason})")

        return {
            'suggested_order': suggested_order,
            'nonlinearity_score': nonlinearity_score,
            'reason': reason
        }

    def _cross_validation_order_selection(self, X, Y, max_order):
        """äº¤å‰éªŒè¯é€‰æ‹©é˜¶æ•°"""
        print("ğŸ”„ äº¤å‰éªŒè¯æµ‹è¯•ä¸åŒé˜¶æ•°...")

        orders = range(1, min(max_order + 1, 6))
        cv_scores = {}

        kf = KFold(n_splits=5, shuffle=True, random_state=42)

        for order in orders:
            scores = []

            for train_idx, val_idx in kf.split(X):
                X_train, X_val = X[train_idx], X[val_idx]
                Y_train, Y_val = Y[train_idx], Y[val_idx]

                try:
                    score = self._fit_evaluate_pce_order(X_train, Y_train, X_val, Y_val, order)
                    scores.append(score)
                except Exception as e:
                    print(f"   è­¦å‘Š: é˜¶æ•°{order}æµ‹è¯•å¤±è´¥: {e}")
                    scores.append(0)

            cv_scores[order] = {
                'mean_score': np.mean(scores),
                'std_score': np.std(scores)
            }

            print(f"   é˜¶æ•°{order}: RÂ² = {np.mean(scores):.3f} Â± {np.std(scores):.3f}")

        best_order = max(cv_scores.keys(), key=lambda k: cv_scores[k]['mean_score'])
        print(f"   äº¤å‰éªŒè¯æœ€ä½³é˜¶æ•°: {best_order}")

        return {
            'best_order': best_order,
            'cv_scores': cv_scores
        }

    def _fit_evaluate_pce_order(self, X_train, Y_train, X_val, Y_val, order):
        """æ‹Ÿåˆå’Œè¯„ä¼°æŒ‡å®šé˜¶æ•°çš„PCEæ¨¡å‹"""
        # åˆ›å»ºå¤šé¡¹å¼ç‰¹å¾
        poly = PolynomialFeatures(degree=order, include_bias=True)
        X_train_poly = poly.fit_transform(X_train)
        X_val_poly = poly.transform(X_val)

        # è®­ç»ƒæ¨¡å‹
        model = Ridge(alpha=1e-6)

        if Y_train.ndim == 1:
            model.fit(X_train_poly, Y_train)
            Y_pred = model.predict(X_val_poly)
            return max(0, r2_score(Y_val, Y_pred))
        else:
            scores = []
            for i in range(Y_train.shape[1]):
                model.fit(X_train_poly, Y_train[:, i])
                Y_pred = model.predict(X_val_poly)
                score = r2_score(Y_val[:, i], Y_pred)
                scores.append(max(0, score))
            return np.mean(scores)

    def _information_criterion_selection(self, X, Y, max_order):
        """ä¿¡æ¯å‡†åˆ™é€‰æ‹©"""
        print("ğŸ“ˆ ä¿¡æ¯å‡†åˆ™åˆ†æ...")

        orders = range(1, min(max_order + 1, 6))
        ic_results = {}

        for order in orders:
            try:
                aic, bic = self._compute_information_criteria(X, Y, order)
                ic_results[order] = {'AIC': aic, 'BIC': bic}
                print(f"   é˜¶æ•°{order}: AIC = {aic:.2f}, BIC = {bic:.2f}")
            except Exception as e:
                print(f"   è­¦å‘Š: é˜¶æ•°{order}ä¿¡æ¯å‡†åˆ™è®¡ç®—å¤±è´¥: {e}")
                ic_results[order] = {'AIC': float('inf'), 'BIC': float('inf')}

        best_aic_order = min(ic_results.keys(), key=lambda k: ic_results[k]['AIC'])
        best_bic_order = min(ic_results.keys(), key=lambda k: ic_results[k]['BIC'])

        print(f"   AICæœ€ä½³é˜¶æ•°: {best_aic_order}")
        print(f"   BICæœ€ä½³é˜¶æ•°: {best_bic_order}")

        return {
            'best_aic_order': best_aic_order,
            'best_bic_order': best_bic_order,
            'ic_results': ic_results
        }

    def _compute_information_criteria(self, X, Y, order):
        """è®¡ç®—AICå’ŒBIC"""
        poly = PolynomialFeatures(degree=order, include_bias=True)
        X_poly = poly.fit_transform(X)

        n_samples, n_features = X_poly.shape
        model = LinearRegression()

        if Y.ndim == 1:
            model.fit(X_poly, Y)
            Y_pred = model.predict(X_poly)
            mse = mean_squared_error(Y, Y_pred)
        else:
            mses = []
            for i in range(Y.shape[1]):
                model.fit(X_poly, Y[:, i])
                Y_pred = model.predict(X_poly)
                mse = mean_squared_error(Y[:, i], Y_pred)
                mses.append(mse)
            mse = np.mean(mses)

        # è®¡ç®—AICå’ŒBIC
        log_likelihood = -n_samples * np.log(mse + 1e-8) / 2
        aic = 2 * n_features - 2 * log_likelihood
        bic = np.log(n_samples) * n_features - 2 * log_likelihood

        return aic, bic

    def _make_final_decision(self, theory_result, cv_result, ic_result):
        """ç»¼åˆå†³ç­–"""
        print("ğŸ¯ ç»¼åˆå†³ç­–...")

        suggestions = [
            theory_result['suggested_order'],
            cv_result['best_order'],
            ic_result['best_aic_order'],
            ic_result['best_bic_order']
        ]

        # è®¡ç®—åŠ æƒå¹³å‡
        weights = [0.4, 0.3, 0.15, 0.15]
        weighted_order = sum(s * w for s, w in zip(suggestions, weights))
        final_order = max(1, int(round(weighted_order)))

        print(f"   ç†è®ºå»ºè®®: {theory_result['suggested_order']}")
        print(f"   äº¤å‰éªŒè¯: {cv_result['best_order']}")
        print(f"   AICå»ºè®®: {ic_result['best_aic_order']}")
        print(f"   BICå»ºè®®: {ic_result['best_bic_order']}")
        print(f"   åŠ æƒå¹³å‡: {weighted_order:.2f}")
        print(f"   æœ€ç»ˆå†³ç­–: {final_order}")

        return final_order
    
    def _compute_basis_functions(self, X):
        """
        è®¡ç®—å¤šé¡¹å¼åŸºå‡½æ•°ï¼ˆæ”¯æŒä»»æ„é˜¶æ•°ï¼‰

        Args:
            X: è¾“å…¥æ•°æ® (n_samples, input_dim)

        Returns:
            basis_matrix: åŸºå‡½æ•°çŸ©é˜µ (n_samples, n_basis)
        """
        # ä½¿ç”¨sklearnçš„PolynomialFeaturesæ¥ç”ŸæˆåŸºå‡½æ•°
        poly = PolynomialFeatures(degree=self.polynomial_order, include_bias=True)
        Phi = poly.fit_transform(X)

        # ç¡®ä¿åŸºå‡½æ•°æ•°é‡åŒ¹é…
        if Phi.shape[1] != self.n_basis:
            print(f"âš ï¸  åŸºå‡½æ•°æ•°é‡è°ƒæ•´: æœŸæœ›{self.n_basis} â†’ å®é™…{Phi.shape[1]}")
            self.n_basis = Phi.shape[1]

        return Phi
    
    def generate_training_data(self, n_samples=1000, noise_level=0.01):
        """
        ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼ˆæ¨¡æ‹Ÿä¸€ä¸ªå¤æ‚çš„éçº¿æ€§å‡½æ•°ï¼‰
        
        Args:
            n_samples: æ ·æœ¬æ•°é‡
            noise_level: å™ªå£°æ°´å¹³
            
        Returns:
            X: è¾“å…¥æ•°æ® (n_samples, input_dim)
            Y: è¾“å‡ºæ•°æ® (n_samples, output_dim)
        """
        print(f"Generating {n_samples} training samples...")
        
        # ç”Ÿæˆéšæœºè¾“å…¥æ•°æ® (åœ¨[-1, 1]èŒƒå›´å†…)
        X = np.random.uniform(-1, 1, (n_samples, self.input_dim))
        
        # ç”Ÿæˆå¤æ‚çš„éçº¿æ€§è¾“å‡º
        Y = np.zeros((n_samples, self.output_dim))
        
        for i in range(n_samples):
            x1, x2 = X[i, 0], X[i, 1]
            
            # ä¸ºæ¯ä¸ªè¾“å‡ºç»´åº¦å®šä¹‰ä¸åŒçš„éçº¿æ€§å‡½æ•°
            for j in range(self.output_dim):
                # åˆ›å»ºå¤æ‚çš„éçº¿æ€§å…³ç³»
                base_func = (
                    0.5 * np.sin(2 * np.pi * x1 + j * 0.1) * np.cos(np.pi * x2) +
                    0.3 * (x1**2 + x2**2) * np.exp(-0.5 * (x1**2 + x2**2)) +
                    0.2 * x1 * x2 * np.sin(j * 0.05) +
                    0.1 * (x1**3 - x2**3) +
                    j * 0.01  # æ·»åŠ è¾“å‡ºç»´åº¦ç›¸å…³çš„åç§»
                )
                
                # æ·»åŠ å™ªå£°
                Y[i, j] = base_func + np.random.normal(0, noise_level)
        
        print(f"Training data generated successfully!")
        print(f"  Input range: [{X.min():.3f}, {X.max():.3f}]")
        print(f"  Output range: [{Y.min():.3f}, {Y.max():.3f}]")
        
        return X, Y
    
    def train(self, X, Y, test_size=0.2, regularization=1e-6, max_order=5):
        """
        è®­ç»ƒPCEæ¨¡å‹ï¼ˆé›†æˆæ™ºèƒ½é˜¶æ•°é€‰æ‹©ï¼‰

        Args:
            X: è¾“å…¥æ•°æ®
            Y: è¾“å‡ºæ•°æ®
            test_size: æµ‹è¯•é›†æ¯”ä¾‹
            regularization: æ­£åˆ™åŒ–å‚æ•°
            max_order: è‡ªåŠ¨é€‰æ‹©æ—¶çš„æœ€å¤§é˜¶æ•°
        """
        print(f"\nğŸš€ å¼€å§‹PCEæ¨¡å‹è®­ç»ƒ (æ ·æœ¬æ•°: {len(X)})")

        # æ™ºèƒ½é˜¶æ•°é€‰æ‹©
        if self.auto_order_selection and self.polynomial_order is None:
            print("\nğŸ“Š å¯åŠ¨æ™ºèƒ½é˜¶æ•°é€‰æ‹©...")
            self.select_optimal_order(X, Y, max_order)
        elif self.polynomial_order is None:
            print("âš ï¸  æœªæŒ‡å®šé˜¶æ•°ä¸”æœªå¯ç”¨è‡ªåŠ¨é€‰æ‹©ï¼Œä½¿ç”¨é»˜è®¤é˜¶æ•°2")
            self.polynomial_order = 2
            self.n_basis = self._calculate_basis_count()

        print(f"\nâœ… ä½¿ç”¨PCEé˜¶æ•°: {self.polynomial_order}")
        print(f"   åŸºå‡½æ•°æ•°é‡: {self.n_basis}")

        # æ£€æŸ¥æ ·æœ¬æ•°é‡å……è¶³æ€§
        min_samples_required = self.n_basis * 5
        if len(X) < min_samples_required:
            print(f"âš ï¸  è­¦å‘Š: æ ·æœ¬æ•°é‡({len(X)})å¯èƒ½ä¸è¶³ï¼Œå»ºè®®è‡³å°‘{min_samples_required}ä¸ªæ ·æœ¬")

        # åˆ†å‰²è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
        X_train, X_test, Y_train, Y_test = train_test_split(
            X, Y, test_size=test_size, random_state=42
        )

        print(f"\nğŸ“Š æ•°æ®åˆ†å‰²:")
        print(f"   è®­ç»ƒæ ·æœ¬: {len(X_train)}")
        print(f"   æµ‹è¯•æ ·æœ¬: {len(X_test)}")

        # æ ‡å‡†åŒ–æ•°æ®
        X_train_scaled = self.input_scaler.fit_transform(X_train)
        X_test_scaled = self.input_scaler.transform(X_test)
        Y_train_scaled = self.output_scaler.fit_transform(Y_train)
        Y_test_scaled = self.output_scaler.transform(Y_test)

        # è®¡ç®—åŸºå‡½æ•°çŸ©é˜µ
        print("\nğŸ”§ è®¡ç®—å¤šé¡¹å¼åŸºå‡½æ•°...")
        Phi_train = self._compute_basis_functions(X_train_scaled)
        Phi_test = self._compute_basis_functions(X_test_scaled)
        
        # ä½¿ç”¨æœ€å°äºŒä¹˜æ³•æ±‚è§£PCEç³»æ•°
        # æ·»åŠ æ­£åˆ™åŒ–ä»¥æé«˜æ•°å€¼ç¨³å®šæ€§
        A = Phi_train.T @ Phi_train + regularization * np.eye(self.n_basis)
        
        self.coefficients = np.zeros((self.output_dim, self.n_basis))
        
        for i in range(self.output_dim):
            b = Phi_train.T @ Y_train_scaled[:, i]
            self.coefficients[i, :] = np.linalg.solve(A, b)
        
        # è¯„ä¼°æ¨¡å‹æ€§èƒ½
        Y_train_pred = self._predict_scaled(X_train_scaled)
        Y_test_pred = self._predict_scaled(X_test_scaled)
        
        # åæ ‡å‡†åŒ–é¢„æµ‹ç»“æœ
        Y_train_pred = self.output_scaler.inverse_transform(Y_train_pred)
        Y_test_pred = self.output_scaler.inverse_transform(Y_test_pred)
        
        # è®¡ç®—è¯¯å·®æŒ‡æ ‡
        train_mse = mean_squared_error(Y_train, Y_train_pred)
        test_mse = mean_squared_error(Y_test, Y_test_pred)
        train_r2 = r2_score(Y_train, Y_train_pred)
        test_r2 = r2_score(Y_test, Y_test_pred)
        
        print(f"Training completed!")
        print(f"  Training MSE: {train_mse:.6f}")
        print(f"  Test MSE: {test_mse:.6f}")
        print(f"  Training R2: {train_r2:.6f}")
        print(f"  Test R2: {test_r2:.6f}")
        
        return {
            'train_mse': train_mse,
            'test_mse': test_mse,
            'train_r2': train_r2,
            'test_r2': test_r2,
            'X_test': X_test,
            'Y_test': Y_test,
            'Y_test_pred': Y_test_pred
        }
    
    def _predict_scaled(self, X_scaled):
        """ä½¿ç”¨æ ‡å‡†åŒ–åçš„è¾“å…¥è¿›è¡Œé¢„æµ‹"""
        Phi = self._compute_basis_functions(X_scaled)
        return Phi @ self.coefficients.T
    
    def predict(self, X):
        """
        ä½¿ç”¨è®­ç»ƒå¥½çš„PCEæ¨¡å‹è¿›è¡Œé¢„æµ‹
        
        Args:
            X: è¾“å…¥æ•°æ®
            
        Returns:
            Y_pred: é¢„æµ‹è¾“å‡º
        """
        if self.coefficients is None:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨train()æ–¹æ³•")
        
        X_scaled = self.input_scaler.transform(X)
        Y_pred_scaled = self._predict_scaled(X_scaled)
        return self.output_scaler.inverse_transform(Y_pred_scaled)
    
    def save_model(self, filename='pce_model.pkl'):
        """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹"""
        model_data = {
            'coefficients': self.coefficients,
            'input_scaler': self.input_scaler,
            'output_scaler': self.output_scaler,
            'input_dim': self.input_dim,
            'output_dim': self.output_dim,
            'polynomial_order': self.polynomial_order,
            'n_basis': self.n_basis
        }
        
        with open(filename, 'wb') as f:
            pickle.dump(model_data, f)
        
        print(f"Model saved to {filename}")
    
    def load_model(self, filename='pce_model.pkl'):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        with open(filename, 'rb') as f:
            model_data = pickle.load(f)
        
        self.coefficients = model_data['coefficients']
        self.input_scaler = model_data['input_scaler']
        self.output_scaler = model_data['output_scaler']
        self.input_dim = model_data['input_dim']
        self.output_dim = model_data['output_dim']
        self.polynomial_order = model_data['polynomial_order']
        self.n_basis = model_data['n_basis']
        
        print(f"Model loaded from {filename}")
    
    def export_fortran_coefficients(self, filename='pce_coefficients.txt'):
        """å¯¼å‡ºç³»æ•°åˆ°Fortranæ ¼å¼çš„æ–‡ä»¶"""
        if self.coefficients is None:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        with open(filename, 'w') as f:
            f.write("! PCE Coefficients for Fortran\n")
            f.write(f"! Input dimension: {self.input_dim}\n")
            f.write(f"! Output dimension: {self.output_dim}\n")
            f.write(f"! Polynomial order: {self.polynomial_order}\n")
            f.write(f"! Number of basis functions: {self.n_basis}\n")
            f.write("!\n")
            f.write("! Coefficients matrix (output_dim x n_basis)\n")
            f.write("real*8 coeff(78,6)\n")
            f.write("data coeff / &\n")
            
            for i in range(self.output_dim):
                line = ", ".join([f"{coeff:.8f}d0" for coeff in self.coefficients[i, :]])
                if i < self.output_dim - 1:
                    f.write(f"  {line}, &\n")
                else:
                    f.write(f"  {line}  &\n")
            f.write("  /\n")
        
        print(f"Fortran coefficients exported to {filename}")

def main():
    """ä¸»å‡½æ•°ï¼šæ¼”ç¤ºPCEè®­ç»ƒè¿‡ç¨‹"""
    print("=" * 60)
    print("PCE Neural Network Replacement Training Demo")
    print("=" * 60)
    
    # åˆ›å»ºPCEè®­ç»ƒå™¨
    trainer = PCETrainer(input_dim=2, output_dim=78, polynomial_order=2)
    
    # ç”Ÿæˆè®­ç»ƒæ•°æ®
    X, Y = trainer.generate_training_data(n_samples=2000, noise_level=0.01)
    
    # è®­ç»ƒæ¨¡å‹
    results = trainer.train(X, Y, test_size=0.2, regularization=1e-6)
    
    # ä¿å­˜æ¨¡å‹
    trainer.save_model('pce_model.pkl')
    
    # å¯¼å‡ºFortranç³»æ•°
    trainer.export_fortran_coefficients('pce_coefficients_new.txt')
    
    # å¯è§†åŒ–ç»“æœï¼ˆä»…æ˜¾ç¤ºå‰å‡ ä¸ªè¾“å‡ºç»´åº¦ï¼‰
    plt.figure(figsize=(15, 5))
    
    for i in range(min(3, trainer.output_dim)):
        plt.subplot(1, 3, i+1)
        plt.scatter(results['Y_test'][:, i], results['Y_test_pred'][:, i], alpha=0.6)
        plt.plot([results['Y_test'][:, i].min(), results['Y_test'][:, i].max()], 
                 [results['Y_test'][:, i].min(), results['Y_test'][:, i].max()], 'r--')
        plt.xlabel(f'True Output {i+1}')
        plt.ylabel(f'Predicted Output {i+1}')
        plt.title(f'Output {i+1} Prediction')
        plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('pce_training_results.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print("\nTraining completed successfully!")
    print("Files generated:")
    print("  - pce_model.pkl: å®Œæ•´çš„Pythonæ¨¡å‹")
    print("  - pce_coefficients_new.txt: Fortranç³»æ•°æ–‡ä»¶")
    print("  - pce_training_results.png: è®­ç»ƒç»“æœå¯è§†åŒ–")

if __name__ == "__main__":
    main()
